{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPi22VbC+ZLSknwKMBqL1KL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/newmantic/GPT/blob/main/GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VOWD_PBv6Xol"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x)"
      ],
      "metadata": {
        "id": "0_bssKzw6jw9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_size, max_len=512):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.encoding = torch.zeros(max_len, embed_size)\n",
        "        pos = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        i = torch.arange(0, embed_size // 2).float()\n",
        "        angle_rates = 1 / (10000 ** (2 * i / embed_size))\n",
        "        self.encoding[:, 0::2] = torch.sin(pos * angle_rates)\n",
        "        self.encoding[:, 1::2] = torch.cos(pos * angle_rates)\n",
        "        self.encoding = self.encoding.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.encoding[:, :x.size(1), :]"
      ],
      "metadata": {
        "id": "TBa_U3X36nNF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        assert self.head_dim * heads == embed_size, \"Embedding size must be divisible by heads\"\n",
        "\n",
        "        self.values = nn.Linear(self.head_dim, self.embed_size, bias=False)\n",
        "        self.keys = nn.Linear(self.head_dim, self.embed_size, bias=False)\n",
        "        self.queries = nn.Linear(self.head_dim, self.embed_size, bias=False)\n",
        "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, values, keys, query, mask):\n",
        "        N = query.shape[0]\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "\n",
        "        # Split the embedding into multiple heads for multi-head attention\n",
        "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
        "\n",
        "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
        "\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
        "\n",
        "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
        "            N, query_len, self.embed_size\n",
        "        )\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "rTNErE7x6p4W"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_size, ff_hidden_size, dropout):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(embed_size, ff_hidden_size)\n",
        "        self.fc2 = nn.Linear(ff_hidden_size, embed_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.dropout(F.relu(self.fc1(x))))"
      ],
      "metadata": {
        "id": "N5U_iCOK6s47"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, ff_hidden_size, dropout):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = SelfAttention(embed_size, heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "        self.ff = FeedForward(embed_size, ff_hidden_size, dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, value, key, query, mask):\n",
        "        attention = self.attention(value, key, query, mask)\n",
        "        x = self.dropout(self.norm1(attention + query))\n",
        "        forward = self.ff(x)\n",
        "        out = self.dropout(self.norm2(forward + x))\n",
        "        return out"
      ],
      "metadata": {
        "id": "0HIQYjeT6vTk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_layers, heads, ff_hidden_size, dropout, max_len):\n",
        "        super(GPT, self).__init__()\n",
        "        self.token_embedding = TokenEmbedding(vocab_size, embed_size)\n",
        "        self.position_encoding = PositionalEncoding(embed_size, max_len)\n",
        "        self.layers = nn.ModuleList(\n",
        "            [TransformerBlock(embed_size, heads, ff_hidden_size, dropout) for _ in range(num_layers)]\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        out = self.token_embedding(x)\n",
        "        out = self.position_encoding(out)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            out = layer(out, out, out, mask)\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "HLkT2VVT6yGL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define a small vocab size and model parameters for the example\n",
        "vocab_size = 10000\n",
        "embed_size = 128\n",
        "num_layers = 2\n",
        "heads = 8\n",
        "ff_hidden_size = 512\n",
        "dropout = 0.3\n",
        "max_len = 512\n",
        "\n",
        "# Instantiate the GPT model\n",
        "model = GPT(vocab_size, embed_size, num_layers, heads, ff_hidden_size, dropout, max_len)\n",
        "\n",
        "# Example input: batch size of 2, sequence length of 10\n",
        "x = torch.randint(0, vocab_size, (2, 10))\n",
        "\n",
        "# No mask for simplicity in this example\n",
        "mask = None\n",
        "\n",
        "# Forward pass through the model\n",
        "output = model(x, mask)\n",
        "\n",
        "# Print the shapes of the input and output tensors to verify\n",
        "print(f\"Input (x): \\n{x}\")\n",
        "print(f\"Model Output: \\n{output}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_61Vdd260tk",
        "outputId": "abd03dd8-7791-4181-cfa6-1dd534d339bc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input (x): \n",
            "tensor([[9781, 5019, 8708,  102, 5421, 6176, 6697, 1499, 7350, 6237],\n",
            "        [6452, 3634, 3411, 9905, 5510, 2496, 1891, 7042, 9769, 9932]])\n",
            "Model Output: \n",
            "tensor([[[-0.5209, -0.1617, -0.4542,  ..., -0.3552,  0.2403, -0.4144],\n",
            "         [ 0.7682,  0.1492,  0.8142,  ...,  0.9354, -0.8658, -1.2806],\n",
            "         [ 1.0452, -0.3901, -0.1208,  ..., -0.2633, -1.0446,  0.7894],\n",
            "         ...,\n",
            "         [ 0.5350, -0.9450,  0.5673,  ...,  0.1257,  0.6041,  0.8342],\n",
            "         [ 0.0353,  0.9506,  0.4805,  ..., -0.2886,  0.4188,  0.2447],\n",
            "         [ 0.2212,  0.1105, -0.1346,  ..., -0.0510, -0.0761, -0.9258]],\n",
            "\n",
            "        [[-0.2333, -1.0283,  0.3152,  ...,  1.1060,  0.1645,  0.4280],\n",
            "         [ 1.1838,  0.6025, -0.8950,  ..., -0.4215, -0.5368, -0.5116],\n",
            "         [-0.1613,  0.9625, -0.3746,  ...,  0.7593,  0.7765, -0.7956],\n",
            "         ...,\n",
            "         [-0.7567, -0.0351, -1.5185,  ..., -0.5424,  0.3597,  0.0461],\n",
            "         [-1.4078,  0.8243, -0.0842,  ...,  0.3214, -0.1682,  0.0677],\n",
            "         [-0.8614, -0.2944,  0.1249,  ...,  1.1145, -1.0287,  0.0033]]],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    }
  ]
}